import requests
import random
import logging
import os
from bs4 import BeautifulSoup
from selenium import webdriver

logging.basicConfig(level=logging.INFO)

def scrape_website(url, visited_links=[]):
    # Check if the link has already been visited
    if url in visited_links:
        logging.info(f'Link already visited: {url}')
        return
    else:
        visited_links.append(url)
        logging.info(f'Visiting link: {url}')

    try:
        # Set up the webdriver
        options = webdriver.ChromeOptions()
        driver = webdriver.Chrome(executable_path='/path/to/chrome', chrome_options=options)

        # Navigate to the website using the webdriver
        driver.get(url)

        # Wait for the page to load
        driver.implicitly_wait(10)

        # Get the HTML content of the page
        html = driver.page_source

        # Parse the HTML content using Beautiful Soup
        soup = BeautifulSoup(html, 'html.parser')

        # Find all the links on the page
        links = soup.find_all('a')

        # Recursively scrape each new link found on the page
        for link in links:
            link_url = link['href']
            scrape_website(link_url, visited_links)

        # Close the webdriver
        driver.close()
    except Exception as e:
        logging.error(f'Error occurred: {e}')

def main():
    # Ask the user how many Chrome drivers to use
    num_drivers = int(input('Enter the number of Chrome drivers to use: '))

    # Create a list of Chrome drivers
    drivers = [webdriver.Chrome(executable_path='/path/to/chrome') for _ in range(num_drivers)]

    # Set the starting URL
    start_url = 'https://www.robur.lol'

    # Scrape the website and store the visited links in a list
    visited_links = []
    scrape_website(start_url, visited_links)

    # Write the visited links to a text file
    with open('results.txt', 'w') as f:
        for link in visited_links:
            f.write(link + '\n')

    # Close the Chrome drivers
    for driver in drivers:
        driver.close()

main()
